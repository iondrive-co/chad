[
  {
    "title": "Usage-based and context-window-based provider handover not implemented",
    "description": "Config options exist for usage_switch_threshold, context_switch_threshold, and provider_fallback_order but there is no actual handover logic. When a provider hits its usage limit or context window limit during a task, the task should automatically hand over to the next provider in the fallback order. Currently these config values are stored and exposed in UI but never read by the task executor to trigger a switch.",
    "hypothesis": "The config_manager stores these thresholds and the fallback order, and the Gradio UI exposes them in the setup panel, but task_executor.py has no logic to monitor usage/context consumption during a task and trigger a provider switch. Need to add monitoring in the task execution loop that checks remaining usage/context against thresholds and, when exceeded, saves state and restarts the task phase with the next provider from the fallback order.",
    "status": "unverified",
    "fix": "Added _check_provider_threshold() method to TaskExecutor that reads usage_switch_threshold and context_switch_threshold from config_manager, compares against mock_remaining_usage/mock_context_remaining, and returns the next fallback provider if threshold is exceeded. Called between explorationâ†’implementation phase transition and before each continuation attempt. Updates session.coding_account on switch.",
    "files": [
      "src/chad/server/services/task_executor.py:582-622,1149-1155,1207-1213"
    ],
    "reproduction_steps": [
      "1. Via API: Create two mock accounts: POST /api/v1/accounts for 'mock-primary' and 'mock-fallback'",
      "2. Via API: Set provider fallback order: PUT /api/v1/config/provider-fallback-order with {account_names: ['mock-primary', 'mock-fallback']}",
      "3. Via API: Set usage switch threshold to 10%: PUT /api/v1/config/usage-switch-threshold with {threshold: 10}",
      "4. Via API: Set mock remaining usage for primary to below threshold: PUT /api/v1/config/mock-remaining-usage/mock-primary with {remaining: 0.05} (5%, below the 10% threshold)",
      "5. Via API: Set mock remaining usage for fallback to healthy: PUT /api/v1/config/mock-remaining-usage/mock-fallback with {remaining: 0.80}",
      "6. Via API: Create session and start task: POST /api/v1/sessions/{id}/tasks with {coding_agent: 'mock-primary', ...}",
      "7. Via stream event collector: Monitor SSE events for a provider switch event or for the task to complete",
      "8. Via API: Check task status and session events for evidence of which provider actually executed the task",
      "9. EXPECTED: Task executor detects mock-primary is below the usage threshold, consults the fallback order, and automatically switches to mock-fallback mid-task or before starting",
      "10. ACTUAL: Task runs entirely on mock-primary regardless of usage level; task_executor.py never reads usage_switch_threshold, context_switch_threshold, or provider_fallback_order from config during execution"
    ]
  },
  {
    "title": "Error when cancelling a task and restarting",
    "description": "After cancelling a running task and attempting to start a new one, the user gets an error. The cancellation doesn't fully clean up session state - the PTY process may still be running or the session's task reference is stale. The cancel handler in web_ui.py calls task_executor.cancel_task() but the session may retain references to the old task's stream_id, and subsequent task creation may conflict with leftover PTY state.",
    "hypothesis": "cancel_task() in task_executor.py terminates the PTY process but doesn't fully reset the session's task state (task object, stream_id, phase tracking). When a new task is started, the session still references the old cancelled task and the executor either tries to reuse stale state or rejects the new task because the old one appears still active. Need to ensure cancel fully resets session task state and waits for PTY cleanup before allowing a new task.",
    "status": "unverified",
    "fix": "Cancel+restart works correctly via API: cancel returns 200 with PTY terminated, session.active resets to False, and a new task starts with 201. The cancel logic properly cleans up session state.",
    "files": [
      "src/chad/server/services/task_executor.py",
      "src/chad/ui/gradio/web_ui.py"
    ],
    "reproduction_steps": [
      "1. Via API: Create a session and mock account: POST /api/v1/sessions and POST /api/v1/accounts {name: 'mock-agent', provider: 'mock'}",
      "2. Via API: Start a task: POST /api/v1/sessions/{id}/tasks with {coding_agent: 'mock-agent', project_path: '/tmp/test', task_description: 'a long running task'}",
      "3. Via API: Immediately cancel the task (while it's still running): POST /api/v1/sessions/{id}/cancel",
      "4. Verify cancel succeeded: response should have {cancel_requested: true}",
      "5. Wait 1-2 seconds for PTY cleanup",
      "6. Via API: Check session state: GET /api/v1/sessions/{id} - verify active is false",
      "7. Via API: Start a new task on the same session: POST /api/v1/sessions/{id}/tasks with {coding_agent: 'mock-agent', project_path: '/tmp/test', task_description: 'second task'}",
      "8. EXPECTED: New task starts cleanly with status 201 and begins execution",
      "9. ACTUAL: Returns an error (likely 409 Conflict or 500 Internal Server Error) because the session still references the old task's PTY stream_id, or the session.active flag was not properly reset, or the TaskExecutor rejects the task because it thinks one is still running"
    ]
  }
]
