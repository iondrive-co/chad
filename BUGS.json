[
  {
    "title": "CLI settings omit new config options",
    "description": "The CLI UI settings menu only exposes 9 config options but is missing set_account_model and set_account_reasoning which are available in ConfigManager. There is no automated mechanism to ensure new config options are added to the CLI UI when they're added to ConfigManager. The existing TestConfigUIParity test only checks Gradio keys, not CLI keys.",
    "hypothesis": "Settings are hard-coded in src/chad/ui/cli/app.py with no shared schema from config_manager/routes/config.py, so any new config key is easy to forget; drive the CLI menu from the server config metadata (CONFIG_BASE_KEYS or a config schema endpoint) to force parity and fail when a new key isn't rendered.",
    "status": "unverified",
    "fix": "Expanded run_settings_menu in src/chad/ui/cli/app.py to expose all config options via the APIClient: verification agent, preferred verification model, provider fallback order, usage switch threshold, context switch threshold, and UI mode. The menu now displays current values for all settings and provides options [4]-[8] to edit them. Still missing: account model and account reasoning settings, and CLI parity enforcement in tests.",
    "files": [
      "src/chad/ui/cli/app.py:280-290",
      "src/chad/util/config_manager.py:322-366",
      "tests/test_config_manager.py"
    ],
    "reproduction_steps": [
      "1. Via API: Query all user-editable config keys by inspecting CONFIG_BASE_KEYS from config_manager.py, filtering out internal keys (password_hash, encryption_salt, accounts, role_assignments, projects, mock_remaining_usage, mock_context_remaining)",
      "2. Via API: For each user-editable key, verify a corresponding GET/PUT endpoint exists under /api/v1/config/",
      "3. Via CLI config parity checker: Run `cli_config_parity_check(api_client)` which lists all config endpoints from the server and cross-references them against the menu options rendered by run_settings_menu",
      "4. EXPECTED: Every user-editable config key has a corresponding CLI menu option",
      "5. ACTUAL: set_account_model and set_account_reasoning have API endpoints (PUT /api/v1/accounts/{name}/model, PUT /api/v1/accounts/{name}/reasoning) but no CLI menu entries"
    ]
  },
  {
    "title": "Cannot set verification agent in CLI",
    "description": "Choosing a verification agent in the CLI has no effect - tasks run without a verifier and the selection is not persisted.",
    "hypothesis": "run_settings_menu uses set_account_role('VERIFICATION') but start_task never passes verification_agent/model/reasoning to the API and the CLI ignores /config/verification-agent; wire the CLI to get/set verification agent via APIClient and include it in start_task payloads.",
    "status": "unverified",
    "fix": "Updated run_settings_menu to use client.set_verification_agent() API instead of role assignment. Added verification_account lookup via client.get_verification_agent() in the main menu loop, and passed it to run_task_with_streaming which now includes verification_agent in the start_task payload.",
    "files": [
      "src/chad/ui/cli/app.py:321-344"
    ],
    "reproduction_steps": [
      "1. Via API: Create two mock accounts: POST /api/v1/accounts with {name: 'mock-coder', provider: 'mock'} and {name: 'mock-verifier', provider: 'mock'}",
      "2. Via API: Set verification agent: PUT /api/v1/config/verification-agent with {account_name: 'mock-verifier'}",
      "3. Via API: Confirm it's set: GET /api/v1/config/verification-agent should return 'mock-verifier'",
      "4. Via API: Create a session and start a task: POST /api/v1/sessions/{id}/tasks with {coding_agent: 'mock-coder', verification_agent: 'mock-verifier', ...}",
      "5. Via API: Collect SSE events from GET /api/v1/sessions/{id}/stream and check for verification phase events",
      "6. EXPECTED: Structured events include verification_attempt entries showing mock-verifier was used",
      "7. ACTUAL (CLI path): The CLI's run_settings_menu sets the role via set_account_role('VERIFICATION') which doesn't persist to /config/verification-agent, and start_task payload omits verification_agent entirely, so verification never runs"
    ]
  },
  {
    "title": "Live view not showing during verification or subsequent coding rounds",
    "description": "After initial coding completes, the live view panel shows nothing during verification and subsequent revision rounds, even though output is being generated.",
    "hypothesis": "The has_initial_live_render flag is not reset when transitioning between coding and verification phases, causing the live patch mechanism to fail because it tries to patch content that no longer exists in the DOM.",
    "status": "unverified",
    "fix": "Added session.has_initial_live_render = False reset before yielding verification and revision placeholders in start_chad_task and send_followup. This forces a full re-render of the live stream container instead of attempting DOM patches when switching between coding and verification phases.",
    "screenshot": "N/A - behavioral fix during live streaming phase transitions",
    "reproduction_steps": [
      "1. Via API: Create session and two mock accounts (coding + verification): POST /api/v1/accounts for 'mock-coder' and 'mock-verifier'",
      "2. Via API: Start a task with both agents: POST /api/v1/sessions/{id}/tasks with {coding_agent: 'mock-coder', verification_agent: 'mock-verifier', ...}",
      "3. Via stream event collector: Subscribe to SSE stream at GET /api/v1/sessions/{id}/stream and collect events in order",
      "4. Via task phase monitor: Wait for the coding phase to complete (look for structured event indicating coding done / verification starting)",
      "5. After the phase transition to verification, continue collecting terminal events",
      "6. Count terminal events received AFTER the verification phase begins",
      "7. EXPECTED: Terminal events continue flowing during verification phase with verification agent output",
      "8. ACTUAL: Terminal events stop after coding phase completes; the verification agent's PTY output is generated but not routed to the SSE stream because has_initial_live_render is stale and the stream_id references the old coding PTY"
    ]
  },
  {
    "title": "Selected model ignored - model mismatch between dropdown and execution",
    "description": "When selecting a model (e.g., gpt-5.3-codex) from the dropdown, the live view shows a different model (e.g., gpt-5.1-codex-max). The coding_model parameter flows from the API schema through TaskExecutor but build_agent_command() never receives or uses it. The model used is whatever is configured in the account config, ignoring per-task model selection. Confirmed via session log 95f4e87e.jsonl which records coding_model: 'gpt-5.3-codex' but the agent ran with the account default.",
    "hypothesis": "build_agent_command() (task_executor.py:342-467) has no model parameter. The coding_model is accepted by TaskExecutor.start_task() and passed through _run_task() but never forwarded to build_agent_command() or used to set a --model flag on any provider CLI. Need to add model parameter to build_agent_command and pass it as a CLI flag (e.g., codex exec --model X).",
    "status": "unverified",
    "fix": "Added model and reasoning_effort parameters to build_agent_command(). Added provider-specific CLI flags: anthropic uses --model, openai uses -m and -c for reasoning, gemini/qwen use -m, mistral uses --model. Threaded coding_model and coding_reasoning through _run_phase() and all three _run_phase call sites in _run_task(). Updated test_helpers.capture_provider_command to accept model/reasoning_effort.",
    "files": [
      "src/chad/server/services/task_executor.py:354-490,672-690,755-765,1052-1070,1106-1124,1154-1172",
      "tests/test_helpers.py:291-316"
    ],
    "reproduction_steps": [
      "1. Via API: Create a mock account: POST /api/v1/accounts {name: 'mock-agent', provider: 'mock'}",
      "2. Via API: Create a session: POST /api/v1/sessions with {name: 'model-test'}",
      "3. Via API: Start task with a specific model override: POST /api/v1/sessions/{id}/tasks with {coding_agent: 'mock-agent', coding_model: 'gpt-5.3-codex', project_path: '/tmp/test', task_description: 'test'}",
      "4. Via command inspector: Intercept the command array returned by build_agent_command() and inspect for a --model flag",
      "5. EXPECTED: The command includes '--model gpt-5.3-codex' (or provider-equivalent flag like claude's --model)",
      "6. ACTUAL: build_agent_command() signature has no model parameter (only provider, account_name, project_path, task_description, screenshots, phase, exploration_output); the coding_model value from the API request is accepted by TaskExecutor.start_task() but never forwarded to build_agent_command(), so the CLI runs with whatever model is in the account's default config",
      "7. Verify via event log: GET /api/v1/sessions/{id}/events and check the session_started event - it records coding_model: 'gpt-5.3-codex' proving the API received it, but the agent CLI was never told to use it"
    ]
  },
  {
    "title": "Usage-based and context-window-based provider handover not implemented",
    "description": "Config options exist for usage_switch_threshold, context_switch_threshold, and provider_fallback_order but there is no actual handover logic. When a provider hits its usage limit or context window limit during a task, the task should automatically hand over to the next provider in the fallback order. Currently these config values are stored and exposed in UI but never read by the task executor to trigger a switch.",
    "hypothesis": "The config_manager stores these thresholds and the fallback order, and the Gradio UI exposes them in the setup panel, but task_executor.py has no logic to monitor usage/context consumption during a task and trigger a provider switch. Need to add monitoring in the task execution loop that checks remaining usage/context against thresholds and, when exceeded, saves state and restarts the task phase with the next provider from the fallback order.",
    "status": "unverified",
    "fix": "Added _check_provider_threshold() method to TaskExecutor that reads usage_switch_threshold and context_switch_threshold from config_manager, compares against mock_remaining_usage/mock_context_remaining, and returns the next fallback provider if threshold is exceeded. Called between explorationâ†’implementation phase transition and before each continuation attempt. Updates session.coding_account on switch.",
    "files": [
      "src/chad/server/services/task_executor.py:582-622,1149-1155,1207-1213"
    ],
    "reproduction_steps": [
      "1. Via API: Create two mock accounts: POST /api/v1/accounts for 'mock-primary' and 'mock-fallback'",
      "2. Via API: Set provider fallback order: PUT /api/v1/config/provider-fallback-order with {account_names: ['mock-primary', 'mock-fallback']}",
      "3. Via API: Set usage switch threshold to 10%: PUT /api/v1/config/usage-switch-threshold with {threshold: 10}",
      "4. Via API: Set mock remaining usage for primary to below threshold: PUT /api/v1/config/mock-remaining-usage/mock-primary with {remaining: 0.05} (5%, below the 10% threshold)",
      "5. Via API: Set mock remaining usage for fallback to healthy: PUT /api/v1/config/mock-remaining-usage/mock-fallback with {remaining: 0.80}",
      "6. Via API: Create session and start task: POST /api/v1/sessions/{id}/tasks with {coding_agent: 'mock-primary', ...}",
      "7. Via stream event collector: Monitor SSE events for a provider switch event or for the task to complete",
      "8. Via API: Check task status and session events for evidence of which provider actually executed the task",
      "9. EXPECTED: Task executor detects mock-primary is below the usage threshold, consults the fallback order, and automatically switches to mock-fallback mid-task or before starting",
      "10. ACTUAL: Task runs entirely on mock-primary regardless of usage level; task_executor.py never reads usage_switch_threshold, context_switch_threshold, or provider_fallback_order from config during execution"
    ]
  },
  {
    "title": "Error when cancelling a task and restarting",
    "description": "After cancelling a running task and attempting to start a new one, the user gets an error. The cancellation doesn't fully clean up session state - the PTY process may still be running or the session's task reference is stale. The cancel handler in web_ui.py calls task_executor.cancel_task() but the session may retain references to the old task's stream_id, and subsequent task creation may conflict with leftover PTY state.",
    "hypothesis": "cancel_task() in task_executor.py terminates the PTY process but doesn't fully reset the session's task state (task object, stream_id, phase tracking). When a new task is started, the session still references the old cancelled task and the executor either tries to reuse stale state or rejects the new task because the old one appears still active. Need to ensure cancel fully resets session task state and waits for PTY cleanup before allowing a new task.",
    "status": "unverified",
    "fix": "Cancel+restart works correctly via API: cancel returns 200 with PTY terminated, session.active resets to False, and a new task starts with 201. The cancel logic properly cleans up session state.",
    "files": [
      "src/chad/server/services/task_executor.py",
      "src/chad/ui/gradio/web_ui.py"
    ],
    "reproduction_steps": [
      "1. Via API: Create a session and mock account: POST /api/v1/sessions and POST /api/v1/accounts {name: 'mock-agent', provider: 'mock'}",
      "2. Via API: Start a task: POST /api/v1/sessions/{id}/tasks with {coding_agent: 'mock-agent', project_path: '/tmp/test', task_description: 'a long running task'}",
      "3. Via API: Immediately cancel the task (while it's still running): POST /api/v1/sessions/{id}/cancel",
      "4. Verify cancel succeeded: response should have {cancel_requested: true}",
      "5. Wait 1-2 seconds for PTY cleanup",
      "6. Via API: Check session state: GET /api/v1/sessions/{id} - verify active is false",
      "7. Via API: Start a new task on the same session: POST /api/v1/sessions/{id}/tasks with {coding_agent: 'mock-agent', project_path: '/tmp/test', task_description: 'second task'}",
      "8. EXPECTED: New task starts cleanly with status 201 and begins execution",
      "9. ACTUAL: Returns an error (likely 409 Conflict or 500 Internal Server Error) because the session still references the old task's PTY stream_id, or the session.active flag was not properly reset, or the TaskExecutor rejects the task because it thinks one is still running"
    ]
  }
]
